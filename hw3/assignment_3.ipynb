{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv0tdboodQ_k"
      },
      "source": [
        "# Welcome to CSE 276F, Assignment 3\n",
        "\n",
        "In assignment 3, we will be exploring Reinforcement Learning. RL provides a general method for learning policies from reward functions. This assignment will start with some basic RL beginning with making a simple grid environemnt followed by solving it with Deep Q Networks (DQN). We will dive deeper into RL for robotics with algorithms like PPO/SAC in the next assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e7efd84-68d0-4453-a400-b3891b8eadec"
      },
      "source": [
        "## Setup Code / Packages\n",
        "\n",
        "If you are running code locally follow the installation instructions for ManiSkill here: https://maniskill.readthedocs.io/en/latest/user_guide/getting_started/installation.html. You will need a CUDA-enabled GPU on a Linux machine for assignments. Windows/Mac have more limited support but are useful if that is the local machine you have access to for local debugging. When building custom environments you can then use a GUI to explore environments which helps the development process.\n",
        "\n",
        "If you are using UCSD Datahub you don't need to do any additional installation steps. You only need to run `pip install --upgrade mani_skill-nightly`\n",
        "\n",
        "If you are using Google Colab make sure to run the two cells below to install all dependencies.\n",
        "\n",
        "If you see an error such as\n",
        "\n",
        "`ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject`\n",
        "\n",
        "simply restart the notebook after running the installation commands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuZHYS95gBQ2",
        "outputId": "26e9ebef-fc82-441f-a5cb-1996c7040862"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libvulkan1\n",
            "Recommended packages:\n",
            "  mesa-vulkan-drivers | vulkan-icd\n",
            "The following NEW packages will be installed:\n",
            "  libvulkan-dev libvulkan1\n",
            "0 upgraded, 2 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 1,020 kB of archives.\n",
            "After this operation, 17.2 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libvulkan1 amd64 1.3.204.1-2 [128 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libvulkan-dev amd64 1.3.204.1-2 [892 kB]\n",
            "Fetched 1,020 kB in 2s (568 kB/s)\n",
            "Selecting previously unselected package libvulkan1:amd64.\n",
            "(Reading database ... 126102 files and directories currently installed.)\n",
            "Preparing to unpack .../libvulkan1_1.3.204.1-2_amd64.deb ...\n",
            "Unpacking libvulkan1:amd64 (1.3.204.1-2) ...\n",
            "Selecting previously unselected package libvulkan-dev:amd64.\n",
            "Preparing to unpack .../libvulkan-dev_1.3.204.1-2_amd64.deb ...\n",
            "Unpacking libvulkan-dev:amd64 (1.3.204.1-2) ...\n",
            "Setting up libvulkan1:amd64 (1.3.204.1-2) ...\n",
            "Setting up libvulkan-dev:amd64 (1.3.204.1-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "Collecting mani_skill-nightly\n",
            "  Downloading mani_skill_nightly-2025.5.22.816-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting numpy<2.0.0,>=1.22 (from mani_skill-nightly)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from mani_skill-nightly) (1.15.3)\n",
            "Collecting dacite (from mani_skill-nightly)\n",
            "  Downloading dacite-1.9.2-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting gymnasium==0.29.1 (from mani_skill-nightly)\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from mani_skill-nightly) (3.13.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from mani_skill-nightly) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from mani_skill-nightly) (4.67.1)\n",
            "Requirement already satisfied: GitPython in /usr/local/lib/python3.11/dist-packages (from mani_skill-nightly) (3.1.44)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from mani_skill-nightly) (0.9.0)\n",
            "Collecting transforms3d (from mani_skill-nightly)\n",
            "  Downloading transforms3d-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting trimesh (from mani_skill-nightly)\n",
            "  Downloading trimesh-4.6.10-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (from mani_skill-nightly) (2.37.0)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.11/dist-packages (from mani_skill-nightly) (7.34.0)\n",
            "Collecting pytorch-kinematics==0.7.5 (from mani_skill-nightly)\n",
            "  Downloading pytorch_kinematics-0.7.5-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (from mani_skill-nightly) (12.0.0)\n",
            "Collecting tyro>=0.8.5 (from mani_skill-nightly)\n",
            "  Downloading tyro-0.9.21-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from mani_skill-nightly) (0.31.2)\n",
            "Collecting mplib==0.1.1 (from mani_skill-nightly)\n",
            "  Downloading mplib-0.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting fast-kinematics==0.2.2 (from mani_skill-nightly)\n",
            "  Downloading fast_kinematics-0.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting sapien>=3.0.0.b1 (from mani_skill-nightly)\n",
            "  Downloading sapien-3.0.0b1-cp311-cp311-manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1->mani_skill-nightly) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1->mani_skill-nightly) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1->mani_skill-nightly) (0.0.4)\n",
            "Collecting toppra>=0.4.0 (from mplib==0.1.1->mani_skill-nightly)\n",
            "  Downloading toppra-0.6.3.tar.gz (256 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.1/256.1 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from pytorch-kinematics==0.7.5->mani_skill-nightly) (1.4.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from pytorch-kinematics==0.7.5->mani_skill-nightly) (5.4.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from pytorch-kinematics==0.7.5->mani_skill-nightly) (2.6.0+cu124)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from pytorch-kinematics==0.7.5->mani_skill-nightly) (3.10.0)\n",
            "Collecting pytorch-seed (from pytorch-kinematics==0.7.5->mani_skill-nightly)\n",
            "  Downloading pytorch_seed-0.2.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting arm-pytorch-utilities (from pytorch-kinematics==0.7.5->mani_skill-nightly)\n",
            "  Downloading arm_pytorch_utilities-0.4.3-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: requests>=2.22 in /usr/local/lib/python3.11/dist-packages (from sapien>=3.0.0.b1->mani_skill-nightly) (2.32.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from sapien>=3.0.0.b1->mani_skill-nightly) (3.4.2)\n",
            "Requirement already satisfied: pyperclip in /usr/local/lib/python3.11/dist-packages (from sapien>=3.0.0.b1->mani_skill-nightly) (1.9.0)\n",
            "Requirement already satisfied: opencv-python>=4.0 in /usr/local/lib/python3.11/dist-packages (from sapien>=3.0.0.b1->mani_skill-nightly) (4.11.0.86)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.8.5->mani_skill-nightly) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.8.5->mani_skill-nightly) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.8.5->mani_skill-nightly)\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.8.5->mani_skill-nightly) (4.4.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython->mani_skill-nightly) (4.0.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->mani_skill-nightly) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->mani_skill-nightly) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->mani_skill-nightly) (24.2)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio->mani_skill-nightly) (11.2.1)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.11/dist-packages (from imageio[ffmpeg]->mani_skill-nightly) (0.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from imageio[ffmpeg]->mani_skill-nightly) (5.9.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from IPython->mani_skill-nightly) (75.2.0)\n",
            "Collecting jedi>=0.16 (from IPython->mani_skill-nightly)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from IPython->mani_skill-nightly) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from IPython->mani_skill-nightly) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from IPython->mani_skill-nightly) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from IPython->mani_skill-nightly) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from IPython->mani_skill-nightly) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from IPython->mani_skill-nightly) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from IPython->mani_skill-nightly) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from IPython->mani_skill-nightly) (4.9.0)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml->mani_skill-nightly) (12.575.51)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython->mani_skill-nightly) (5.0.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->IPython->mani_skill-nightly) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->IPython->mani_skill-nightly) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->mani_skill-nightly) (0.2.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22->sapien>=3.0.0.b1->mani_skill-nightly) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22->sapien>=3.0.0.b1->mani_skill-nightly) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22->sapien>=3.0.0.b1->mani_skill-nightly) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22->sapien>=3.0.0.b1->mani_skill-nightly) (2025.4.26)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro>=0.8.5->mani_skill-nightly) (3.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pytorch-kinematics==0.7.5->mani_skill-nightly) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pytorch-kinematics==0.7.5->mani_skill-nightly) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pytorch-kinematics==0.7.5->mani_skill-nightly) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pytorch-kinematics==0.7.5->mani_skill-nightly) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pytorch-kinematics==0.7.5->mani_skill-nightly) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pytorch-kinematics==0.7.5->mani_skill-nightly) (2.9.0.post0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-kinematics==0.7.5->mani_skill-nightly) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->pytorch-kinematics==0.7.5->mani_skill-nightly)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->pytorch-kinematics==0.7.5->mani_skill-nightly)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->pytorch-kinematics==0.7.5->mani_skill-nightly)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->pytorch-kinematics==0.7.5->mani_skill-nightly)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->pytorch-kinematics==0.7.5->mani_skill-nightly)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->pytorch-kinematics==0.7.5->mani_skill-nightly)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->pytorch-kinematics==0.7.5->mani_skill-nightly)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->pytorch-kinematics==0.7.5->mani_skill-nightly)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->pytorch-kinematics==0.7.5->mani_skill-nightly)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-kinematics==0.7.5->mani_skill-nightly) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-kinematics==0.7.5->mani_skill-nightly) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-kinematics==0.7.5->mani_skill-nightly) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->pytorch-kinematics==0.7.5->mani_skill-nightly)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-kinematics==0.7.5->mani_skill-nightly) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-kinematics==0.7.5->mani_skill-nightly) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->pytorch-kinematics==0.7.5->mani_skill-nightly) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.8.5->mani_skill-nightly) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->pytorch-kinematics==0.7.5->mani_skill-nightly) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->pytorch-kinematics==0.7.5->mani_skill-nightly) (3.0.2)\n",
            "Downloading mani_skill_nightly-2025.5.22.816-py3-none-any.whl (101.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fast_kinematics-0.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (624 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.7/624.7 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mplib-0.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_kinematics-0.7.5-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sapien-3.0.0b1-cp311-cp311-manylinux2014_x86_64.whl (49.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transforms3d-0.4.2-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.9.21-py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.3/125.3 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dacite-1.9.2-py3-none-any.whl (16 kB)\n",
            "Downloading trimesh-4.6.10-py3-none-any.whl (711 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m711.2/711.2 kB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Downloading arm_pytorch_utilities-0.4.3-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_seed-0.2.0-py3-none-any.whl (4.2 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: toppra\n",
            "  Building wheel for toppra (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for toppra: filename=toppra-0.6.3-cp311-cp311-linux_x86_64.whl size=776157 sha256=5103b45bbc410ab76c89dd8a6ffb813c104b3afa3c6e8728d3fe9d8632f7064c\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/59/c1/4daf9fdaf34e03a2d94430efe8b070346c86b15bc9d446c162\n",
            "Successfully built toppra\n",
            "Installing collected packages: shtab, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, jedi, fast-kinematics, dacite, trimesh, transforms3d, nvidia-cusparse-cu12, nvidia-cudnn-cu12, gymnasium, tyro, sapien, nvidia-cusolver-cu12, toppra, pytorch-seed, mplib, arm-pytorch-utilities, pytorch-kinematics, mani_skill-nightly\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: gymnasium\n",
            "    Found existing installation: gymnasium 1.1.1\n",
            "    Uninstalling gymnasium-1.1.1:\n",
            "      Successfully uninstalled gymnasium-1.1.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed arm-pytorch-utilities-0.4.3 dacite-1.9.2 fast-kinematics-0.2.2 gymnasium-0.29.1 jedi-0.19.2 mani_skill-nightly-2025.5.22.816 mplib-0.1.1 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-kinematics-0.7.5 pytorch-seed-0.2.0 sapien-3.0.0b1 shtab-1.7.2 toppra-0.6.3 transforms3d-0.4.2 trimesh-4.6.10 tyro-0.9.21\n"
          ]
        }
      ],
      "source": [
        "# setup vulkan\n",
        "!mkdir -p /usr/share/vulkan/icd.d\n",
        "!wget -q https://raw.githubusercontent.com/haosulab/ManiSkill/main/docker/nvidia_icd.json\n",
        "!wget -q https://raw.githubusercontent.com/haosulab/ManiSkill/main/docker/10_nvidia.json\n",
        "!mv nvidia_icd.json /usr/share/vulkan/icd.d\n",
        "!mv 10_nvidia.json /usr/share/glvnd/egl_vendor.d/10_nvidia.json\n",
        "!apt-get install -y --no-install-recommends libvulkan-dev\n",
        "# dependencies\n",
        "!pip install --upgrade mani_skill-nightly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6i6VVRfgCXj"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    import site\n",
        "    site.main() # run this so local pip installs are recognized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NHolc8agFlu"
      },
      "source": [
        "## 1 Basic RL Programming (10 pts)\n",
        "\n",
        "This section will take you through the basics of RL programming/engineering and apply theory to practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6El8OhF1gLTX"
      },
      "source": [
        "### Problem 1.1: Building an environment, \"Avoid the Lava\" (3 pts)\n",
        "\n",
        "We will start with building a simple and visualizable environment called AvoidLava where a controllable agent on a 2D grid of tiles that can be empty or lava, and must find its way to the goal from a starting location.\n",
        "\n",
        "The standard environment interface is typically the [Gym/Gymnasium](https://gymnasium.farama.org/) interface. By adhering to this interface, a lot of RL libraries can work nearly out of the box on your custom environment. In the first assignment you had some experience using Gymnasium through ManiSkill. This section will task you to create a discrete action-space environment using Gymnasium alone without an additional framework.\n",
        "\n",
        "---\n",
        "\n",
        "For this task you need to implement the environment dynamics in the `step()` function, implement environment termination in `get_terminated()`, and implement your own reward function in `get_reward()`.\n",
        "\n",
        "In the Avoid the Lava game, the agent can take 4 discrete actions labelled 0, 1, 2, 3 (go up, right, down, left) on a 2D grid map and the agent position changes to reflect the action taken.\n",
        "\n",
        "If the agent tries to go to a map tile off screen (beyond the map size) nothing should happen, the agent position is unchanged.\n",
        "\n",
        "If the agent takes an action that leads it to end up on a map tile that is on lava, `get_terminated()` should return True.\n",
        "\n",
        "\n",
        "The reward function can be chosen by you, you must define it in `get_reward()`. This reward function must be sufficiently informative/dense such that the agent is encouraged to go towards the goal location and avoids losing due to falling in lava."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYvJoZjbxn4D"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'moviepy.editor'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmoviepy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meditor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ImageSequenceClip\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# this map should be left unchanged\u001b[39;00m\n\u001b[1;32m      7\u001b[0m DEFAULT_LAVA_MAP \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\n\u001b[1;32m      8\u001b[0m     [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m3\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m], [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]\n\u001b[1;32m      9\u001b[0m ])\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'moviepy.editor'"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from moviepy import ImageSequenceClip\n",
        "\n",
        "# this map should be left unchanged\n",
        "DEFAULT_LAVA_MAP = np.array([\n",
        "    [0,3], [1, 1], [2, 1], [2, 3], [3, 3], [3, 4]\n",
        "])\n",
        "class AvoidLava(gym.Env):\n",
        "    def __init__(self, map_size: int = 5, lava_positions = DEFAULT_LAVA_MAP, **kwargs):\n",
        "        self.map_size = map_size\n",
        "        # define obs and action space to let libraries know what to expect as observation data\n",
        "        # and what to provide as action data. We do this for you.\n",
        "        self.lava_positions = lava_positions.copy()\n",
        "        self.observation_space = gym.spaces.Box(0, map_size, shape=(2, ))\n",
        "        self.action_space = gym.spaces.Discrete(4)\n",
        "    def reset(self, seed=None, options=None):\n",
        "        # reset the environment by generating a new map, goal location\n",
        "        # and spawn agent at a fixed start location\n",
        "        self.map_state = np.zeros((self.map_size, self.map_size))\n",
        "        self.agent_pos = np.zeros(2, dtype=int)\n",
        "        self.goal_pos = np.array([\n",
        "            0, 4\n",
        "        ])\n",
        "\n",
        "        return self.get_obs(), {}\n",
        "\n",
        "    def get_obs(self):\n",
        "        return self.agent_pos.copy()\n",
        "    def get_terminated(self):\n",
        "        ### your code goes below ###\n",
        "        # Implement the env termination function, should return True/False\n",
        "        return True\n",
        "        ### your code goes above ###\n",
        "    def get_reward(self):\n",
        "        ### your code goes below ###\n",
        "        # Write the env reward function, should return a float ###\n",
        "        return 0\n",
        "        ### your code goes above ###\n",
        "    def step(self, action):\n",
        "        ### your code goes below ###\n",
        "        # Write the AvoidLava dynamics function below ###\n",
        "        # You can do it by simply updating self.agent_pos appropriately\n",
        "\n",
        "        # action definition. (0, 0) is bottom left, (self.map_size - 1, self.map_size) is top right corner of the map.\n",
        "        # 0: go up\n",
        "        # 1: go right\n",
        "        # 2: go down\n",
        "        # 3: go left\n",
        "\n",
        "        ### your code goes above ###\n",
        "\n",
        "        truncated = False\n",
        "        return self.get_obs(), self.get_reward(), self.get_terminated(), truncated, {}\n",
        "    def render(self):\n",
        "        # very simple visualization code which returns a RGB array to visualize\n",
        "        # the current state\n",
        "        scale = 32\n",
        "        img = np.zeros((self.map_size * scale, self.map_size * scale, 3), dtype=int) + 255\n",
        "        def draw_cube(x, y, color, l=1):\n",
        "          img[img.shape[0] - scale - y*scale:img.shape[0] - scale-y*scale+l, x*scale:x*scale+l] = color\n",
        "        for lava_pos in self.lava_positions:\n",
        "            draw_cube(lava_pos[0], lava_pos[1], color=np.array([252, 81, 48]), l=scale)\n",
        "        draw_cube(self.goal_pos[0], self.goal_pos[1], color=np.array([35, 187, 74]), l=scale)\n",
        "        k = scale//2\n",
        "        img[img.shape[0] - self.agent_pos[1]*scale-2*k+k//2:img.shape[0] - self.agent_pos[1]*scale-k+k//2,\n",
        "            self.agent_pos[0]*scale+k//2:self.agent_pos[0]*scale+k+k//2] = np.array([25, 146, 237])\n",
        "        return img\n",
        "\n",
        "# Code below is just for recording videos to watch agents play the game\n",
        "def record_episode(imgs, file, fps=5):\n",
        "    from pathlib import Path\n",
        "    import os.path as osp\n",
        "    Path(osp.dirname(file)).mkdir(exist_ok=True, parents=True)\n",
        "    clip = ImageSequenceClip(imgs, fps=fps)\n",
        "    clip.write_videofile(file, logger=None)\n",
        "def display_video(path):\n",
        "    from IPython.display import HTML\n",
        "    from base64 import b64encode\n",
        "    mp4 = open(path,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    return HTML(\"\"\"\n",
        "    <video width=400 controls>\n",
        "          <source src=\"%s\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKiIFoVd3HOO",
        "outputId": "b69479f0-e981-4769-f51c-8a16c5fe9e8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([0, 0]), {})"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# we can create our environment and also specify the max number of episodes before the environment should reset\n",
        "env = AvoidLava(render_mode=\"rgb_array\")\n",
        "env = gym.wrappers.TimeLimit(env, max_episode_steps=20)\n",
        "\n",
        "# before using the environment you must reset at least once\n",
        "env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvOIUIX6IGXS"
      },
      "source": [
        "If you implemented your code correctly, the visualization of the initial environment state should look like below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "Drox4R2I7sQV",
        "outputId": "b199ae4b-2b7e-4a7b-e042-28f614a7c089"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7cba9ae86c90>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(env.render())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pcb3vggFINi6"
      },
      "outputs": [],
      "source": [
        "#@markdown Test with asserts that the environment dynamics behave correctly. DO NO CHANGE THIS CODE, simply run it before submitting\n",
        "P1_1_PASS = False\n",
        "assert len(env.reset()) == 2, \"should return obs and info\"\n",
        "assert (env.unwrapped.agent_pos == np.array([0, 0])).all()\n",
        "\n",
        "env.action_space.seed(1)\n",
        "env.step(env.action_space.sample())\n",
        "env.step(env.action_space.sample())\n",
        "assert (env.unwrapped.agent_pos == np.array([1, 0])).all()\n",
        "\n",
        "env.reset()\n",
        "env.action_space.seed(1)\n",
        "for _ in range(20):\n",
        "    obs, _, terminated, truncated, info = env.step(2)\n",
        "assert truncated == True, \"time limit not working\"\n",
        "\n",
        "env.reset()\n",
        "env.action_space.seed(1)\n",
        "obs, rew1, terminated, truncated, info = env.step(0)\n",
        "obs, rew1, terminated, truncated, info = env.step(1)\n",
        "assert terminated == True, \"termination not working\"\n",
        "\n",
        "env.reset()\n",
        "env.action_space.seed(1)\n",
        "env.unwrapped.agent_pos = env.unwrapped.goal_pos.copy()\n",
        "obs, rew2, terminated, truncated, info = env.step(0)\n",
        "assert terminated == False, \"termination at goal not working. should not terminated at goal\"\n",
        "\n",
        "assert rew2 > rew1, \"it seems the reward of the agent landing on a goal is smaller than landing on lava. This reward function is very likely wrong and won't work.\"\n",
        "print(\"Environment is implemented correctly!\")\n",
        "P1_1_PASS = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiXMTUbuH3Iy"
      },
      "source": [
        "A general environment loop looks like the below. We constantly sample actions whether randomly via the action space object or via a policy (we will make one later). An environment needs to be reset when either terminated or truncated are True, meaning either the agent has failed (hit lava) or has won (reached the goal). Ultimately the reward determines how well the agent is doing. The code also saves the first 20 frames as a video that you can watch with the tools shown below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbVzE5nlF2Sr"
      },
      "outputs": [],
      "source": [
        "env.reset()\n",
        "imgs = []\n",
        "imgs.append(env.render())\n",
        "for i in range(20):\n",
        "    obs, rew, terminated, truncated, info = env.step(env.action_space.sample())\n",
        "    imgs.append(env.render())\n",
        "    done = terminated or truncated\n",
        "    if done:\n",
        "        env.reset()\n",
        "        imgs.append(env.render())\n",
        "record_episode(imgs[:20], \"result.mp4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "2IItjLWZYdE0",
        "outputId": "7c0d6a38-db16-4146-a9a0-6fa1fb5413d7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <video width=400 controls>\n",
              "          <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
              "    </video>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "display_video(\"result.mp4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgsogeMhMdky"
      },
      "source": [
        "### Problem 1.2: Learning to play AvoidLava with Tabular Q-Learning (5 pts)\n",
        "\n",
        "Here you have to implement a simple Q-Learning algorithm to learn to play the game.\n",
        "\n",
        "It is up to you to figure how to implement the algorithm from course lecture(s) in code below, some of the skeleton has already been made for you. You get full points for working code implementation that results in a successful evaluation of the agent.\n",
        "\n",
        "The solution implementation with lightly tuned hyperparameters took around 40,000 update iterations of Q-Learning / 5 seconds of time to learn to be able to solve the game from any starting agent position (despite only ever resetting to the same initial agent position).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b9163e69db824736ab0f7d354fbe2216",
            "010d0b81f97d4a93be83f60aa38ba601",
            "32a95d9cd5ef436ba6d8fc847392a66c",
            "677e3400ddb241fba6a5107252e1b3b2",
            "c395959e4c0d4a1ca1b566115d12b220",
            "d4506ddde1ad43468e512b66b0077a39",
            "a1b34ccdbcea4ae5a7dd0b8b00e533e7",
            "b027b36b9a5748c3aae863acb0c247af",
            "bdd40b0f943448389276d18fdb74188a",
            "79c35cd2e0ea403d9a94e98ccdb77f94",
            "130b1ed3a097496fb0adea4ef48bed19"
          ]
        },
        "id": "QnxbnE7CNCWp",
        "outputId": "b410550c-68f7-4066-896e-40262714f608"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9163e69db824736ab0f7d354fbe2216",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/40000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "env = AvoidLava(render_mode=\"rgb_array\")\n",
        "env = gym.wrappers.TimeLimit(env, max_episode_steps=20)\n",
        "\n",
        "### your code goes below ###\n",
        "# Create an appropriate matrix (recommended) or dictionary below so that you can query and set Q(S, A), the state-action value. ###\n",
        "# Be wary about what the shape of the matrix should be, it should encapsulate all possible observation (state), action pairs\n",
        "q_mat = None\n",
        "### your code goes above ###\n",
        "\n",
        "np.random.seed(0)\n",
        "env.action_space.seed(0)\n",
        "obs, _ = env.reset() # obs is s_0\n",
        "\n",
        "# Hyperparameters of Q-Learning algorithm\n",
        "step_size = 0.25\n",
        "discount = 0.95\n",
        "N = 40000\n",
        "for i in tqdm(range(N)):\n",
        "    ### your code goes below ###\n",
        "    # Write an action sampling strategy from lecture or make up your own ###\n",
        "    # You may not implement a ground truth action sampler (e.g. BFS)\n",
        "    # You may not modify environment state directly of env, you can only use env.step and env.reset\n",
        "    action = env.action_space.sample() # you can use this to sample actions uniformly\n",
        "    ### your code goes above ###\n",
        "\n",
        "    next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    ### your code goes below ###\n",
        "    # Implement TD(0) learning here by updating the value function (otherwise known as q_mat as a tabular form)\n",
        "    # Hints: obs is s_t, next_obs is s_{t+1}\n",
        "\n",
        "    ### your code goes above ###\n",
        "\n",
        "    done = terminated or truncated\n",
        "    obs = next_obs\n",
        "    if done:\n",
        "        obs, _ = env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spg2pVGoNn_P"
      },
      "source": [
        "### Problem 1.3: Visualizing the RL Agent\n",
        "\n",
        "This section has code to be filled out to visualize the behavior of the greedy RL agent acting on a trained Q function as well as visualize what the Q function has learned via a saliency map."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gAbG6gkrC_a"
      },
      "source": [
        "#### Visualization of Q Function (1pt):\n",
        "\n",
        "Visualize the Q Function as a heatmap of the game map with cell value equal to $\\max_a {Q(s, a)}$ for cell $s$ in the map (it should be 5x5). You may or may not need to invert the y-axis, the map should resemble the actual gamemap in a way. You may find it interesting to also look at how your q function evolves during the course of training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwRiLuImVWn2"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1)\n",
        "### your code goes below ###\n",
        "### your code goes above ###\n",
        "fig.savefig(\"P2_3.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vq6XKiufKM8"
      },
      "source": [
        "#### Evaluation videos from different starting positions (1pt):\n",
        "\n",
        "Using your trained Q function from earlier, fill in the code below to generate 3 videos of your agent playing the game. It must be trained sufficiently long enough such that in all trials below it solves the game each time. An `assert` is placed to check if the agent solved the task or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPVAeTmmdEfL"
      },
      "outputs": [],
      "source": [
        "for trial, start_pos in enumerate([(0, 0), (3, 0), (4, 3)]):\n",
        "    obs, _ = env.reset()\n",
        "    env.unwrapped.agent_pos = np.array(start_pos) # not good RL practice but for the purpose of homework we set it like this\n",
        "    obs = env.unwrapped.get_obs()\n",
        "    imgs = []\n",
        "    imgs.append(env.render())\n",
        "\n",
        "    ### your code goes below ###\n",
        "    # Using the environment loop and your q function, record videos of your agents\n",
        "    # behavior below with the agent in different starting positions.\n",
        "\n",
        "    ### your code goes above ###\n",
        "\n",
        "    record_episode(imgs, f\"p_1_3_trial_{trial}.mp4\")\n",
        "    # do not remove the assert here\n",
        "    assert (env.unwrapped.agent_pos == env.unwrapped.goal_pos).all(), \"agent was not succesful\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgHeQfm3dz4T"
      },
      "outputs": [],
      "source": [
        "display_video(\"p_1_3_trial_0.mp4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MgJiLnkeNHb"
      },
      "outputs": [],
      "source": [
        "display_video(\"p_1_3_trial_1.mp4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izChxOchePsR"
      },
      "outputs": [],
      "source": [
        "display_video(\"p_1_3_trial_2.mp4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uypxaf_1oNb0"
      },
      "source": [
        "## 2 Deep Reinforcement Learning (10 pts)\n",
        "\n",
        "This section will take you through the basics of deep reinforcement learning and how we evolve from tabular learning methods done in section 2 to more modern deep RL methods that can solve far more complex environments.\n",
        "\n",
        "We will tackle a classic control problem known as CartPole via a Deep Q-Network. The objective is to move the cart left/right to keep the pole upright and not to let it fall.\n",
        "\n",
        "Note that this environment has a different setup compared to the lava game, and further more its default time limit is 500 steps compared to the 20 we used for the lava game (this long time limit may be something of note for DQN training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "EXgGJ4q5oYcT",
        "outputId": "25748739-30d0-4121-bd1b-bae4cacf09fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observation Space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
            "Action Space: Discrete(2)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7cbb66a7c4d0>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "env.reset()\n",
        "print(\"Observation Space:\", env.observation_space)\n",
        "print(\"Action Space:\", env.action_space)\n",
        "plt.imshow(env.render())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsyE8kRuuU-e"
      },
      "source": [
        "Notably the observation space is continuous (Box space), meaning tabular methods will likely struggle / are non-trivial to write. This is where deep learning shines, neural networks are great function approximators and overfitting seen data, we can use those to replace our previous matrix-table backed Q function in Problem 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fVZooequlnt"
      },
      "source": [
        "### Problem 2.1 Building the Neural Nets (2pts)\n",
        "\n",
        "We will use PyTorch for this course, first implement a basic neural network with PyTorch that can take in observations from the gym environment, pass it through a deep network and generate a scalar value for every possible discrete action representing a predicted Q value.\n",
        "\n",
        "A GPU is not necessary here, it will run very fast."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PRIwFeAoaFa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "class QNet(nn.Module):\n",
        "    ### your code goes below ###\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, obs):\n",
        "        pass\n",
        "\n",
        "    ### your code goes above ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHGJOo5MvWUo"
      },
      "outputs": [],
      "source": [
        "# quick sanity test\n",
        "P2_1_PASS = False\n",
        "q_function = QNet()\n",
        "obs, _ = env.reset()\n",
        "obs = torch.from_numpy(obs)\n",
        "assert q_function(obs).shape == (2, )\n",
        "P2_1_PASS = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99yTPBzLwX7H"
      },
      "source": [
        "### Problem 2.2 Building a ReplayBuffer (2pts)\n",
        "\n",
        "Fundamental to DQN and many modern deep RL algorithms is the replay buffer, a object to effectively hold past seen environment transitions to then train on in the future. Implement the code below to enable insertion of environment transition data (observation, next_observation, action, reward, done) and sampling batches of data for training.\n",
        "\n",
        "If you aren't sure about the shapes, play around with env.step() return values. Note that `done = terminated or truncated`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyk72ez5xEWZ"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_size: int, obs_space: gym.spaces.Space, action_space: gym.spaces.Space):\n",
        "        self.buffer_size = buffer_size\n",
        "\n",
        "        ### your code goes below ###\n",
        "        # You may want to add code here to initialize some things\n",
        "\n",
        "        ### your code goes above ###\n",
        "\n",
        "    def store(self, obs, next_obs, reward, action, done):\n",
        "        ### your code goes below ###\n",
        "\n",
        "        ### your code goes above ###\n",
        "        return\n",
        "\n",
        "    def sample(self, batch_size: int):\n",
        "        obs, next_obs, rewards, actions, dones = None, None, None, None, None\n",
        "        ### your code goes below ###\n",
        "        # Should return a batch of observations, next_obs, rewards, actions, and dones as a tuple\n",
        "\n",
        "        ### your code goes above ###\n",
        "        return torch.from_numpy(obs).float(), torch.from_numpy(next_obs).float(), torch.from_numpy(rewards).float(), torch.from_numpy(actions), torch.from_numpy(dones)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFccS6YaxihJ"
      },
      "outputs": [],
      "source": [
        "# quick sanity check for ReplayBuffer\n",
        "P2_2_PASS = False\n",
        "buffer = ReplayBuffer(4, env.observation_space, env.action_space)\n",
        "obs, _ = env.reset()\n",
        "action = env.action_space.sample()\n",
        "for _ in range(5):\n",
        "    next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "    buffer.store(obs, next_obs, reward, action, terminated or truncated)\n",
        "    obs = next_obs\n",
        "b_obs, b_next_obs, b_rewards, b_actions, b_dones = buffer.sample(batch_size=2)\n",
        "assert b_obs.shape == (2, *obs.shape), b_next_obs.shape == (2, *obs.shape)\n",
        "assert b_rewards.shape == (2, )\n",
        "assert b_actions.shape == (2, )\n",
        "P2_2_PASS = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RfaX78czNNW"
      },
      "source": [
        "### Problem 2.3: DQN Training (6pts)\n",
        "Your task here is to implement code below to train a Deep Q Network (DQN). DQN isn't explicitly covered in lecture but we describe the pseudocode below which should have aspects that look similar to lecture content\n",
        "\n",
        "DQN\n",
        "1. Initialize the replay buffer $D$ and Q network $Q$ with two sets of weights, $\\theta$ and $\\theta'$ which are initially equal. $\\theta$ is the behavior policy's weights, and $\\theta'$ is for the target network.\n",
        "2. At each environment timestep:\n",
        "    1. Let $s$ be the current state/observation. With probability $\\epsilon$ sample a random action $a$. Otherwise greedily select $a = \\arg \\max_a Q_\\theta(s, a)$\n",
        "    2. Execute $a$ in the environment and receive reward $r$ and next state/observation $s'$.\n",
        "    3. Add transition $(s, a, s', r)$ to $D$.\n",
        "    4. Every $i$ environment steps (the behavior network update frequency), sample batch from $D$ and compute the TD loss. The TD loss is defined as $TD_\\theta(s, a, s', r)= ||Q_\\theta(s, a) - [r + \\gamma_{a'} \\max Q'_\\theta (s', a')]||^2$.\n",
        "    5. Note that in practice, environments might terminate/truncate (when env is done). In those situations the $\\gamma_{a'} \\max Q'_\\theta (s', a')$ term should be set to 0.\n",
        "    6. Perform gradient descent\n",
        "    7. Every $j$ environment steps (the target network update frequency), set $\\theta'$ equal to $\\theta$.\n",
        "    8. Update $\\epsilon$ (typically to smaller and smaller values to encourage agent to take more greedy actions over time).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Training should generally take about a minute if implemented correctly and using not too large of a neural network and training batch size.\n",
        "\n",
        "Part of this task is to have a general understanding of how to tune RL algorithms, it is a highly practical skill to have for future projects and robot learning in general, especially since RL algorithms are often very black-box. The given hyperparameters are not tuned. They will work, but you will need to modify them a little in order to train a strong enough agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "0046d2d38f7841a1a3e6cca1453aa36e",
            "7f551383d0334b5faafffe973c2c90ee",
            "cb4c13f69877491e9c86f51f4994e594",
            "575332b205b84b089641fb4a3503d96d",
            "ae418a7507bd4172a1c2b46a5b466a00",
            "77fb60f7840c4abd8102c3baa2b0a740",
            "bf256bba7c924167955f49de330ffaae",
            "b426bd2faa4241b789c7923789df845e",
            "160a04d02bc342f58acff4c1b6340463",
            "13cc908517cb4bba98c00ef5b098cc86",
            "667a2617238747e58a6ccd181c266cba"
          ]
        },
        "id": "1g9epmQ4zMxg",
        "outputId": "9762afe8-87ef-4af9-9e15-252fe349537b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0046d2d38f7841a1a3e6cca1453aa36e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/50000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step: 0, eval_eps_ret: 9.0\n",
            "step: 10000, eval_eps_ret: 207.0\n",
            "step: 20000, eval_eps_ret: 298.0\n",
            "step: 30000, eval_eps_ret: 147.0\n",
            "step: 40000, eval_eps_ret: 398.0\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(3)\n",
        "torch.manual_seed(3)\n",
        "### your code goes below ###\n",
        "# tune these hyperparameters, the ones provided work but train slowly\n",
        "buffer_size = 32\n",
        "batch_size = 4\n",
        "discount = 0.99\n",
        "learning_starts = buffer_size\n",
        "training_iterations = 50_000 # for grading, this may not be changed\n",
        "learning_rate = 2.5e-4\n",
        "behavior_net_update_freq = 10\n",
        "target_net_update_freq = 500\n",
        "### your code goes above ###\n",
        "\n",
        "# Create a training env and eval env\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "eval_env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "# Create behavior net and target net\n",
        "behavior_net = QNet()\n",
        "target_net = QNet()\n",
        "target_net.load_state_dict(behavior_net.state_dict())\n",
        "optimizer = optim.Adam(behavior_net.parameters(), lr=learning_rate)\n",
        "\n",
        "# Create replay buffer\n",
        "buffer = ReplayBuffer(buffer_size, env.observation_space, env.action_space)\n",
        "\n",
        "def evaluate(save_video: str = None):\n",
        "    # Evaluates the current agent and report the return and episode length\n",
        "    eps_ret = 0\n",
        "    eps_len = 0\n",
        "    done = False\n",
        "    obs, _ = eval_env.reset()\n",
        "    imgs = []\n",
        "    while not done:\n",
        "        obs, rew, terminated, truncated, info = eval_env.step(behavior_net(torch.from_numpy(obs)).argmax().cpu().numpy())\n",
        "        if save_video is not None:\n",
        "            imgs.append(eval_env.render())\n",
        "        eps_ret += rew\n",
        "        eps_len += 1\n",
        "        done = terminated or truncated\n",
        "        if done:\n",
        "            break\n",
        "    if save_video is not None:\n",
        "        record_episode(imgs, save_video, fps=60)\n",
        "    return eps_ret, eps_len\n",
        "\n",
        "def get_epsilon(step):\n",
        "    ### your code goes below ###\n",
        "    # Implement a custom schedule to reduce epsilon over time so that the RL agent\n",
        "    # takes greedy actions more often\n",
        "    epsilon = 0\n",
        "    ### your code goes above ###\n",
        "    return epsilon\n",
        "\n",
        "pbar = tqdm(range(training_iterations))\n",
        "\n",
        "obs, _ = env.reset()\n",
        "for step in pbar:\n",
        "    ### your code goes below ###\n",
        "    # sample actions\n",
        "    action = env.action_space.sample()\n",
        "    ### your code goes above ###\n",
        "\n",
        "    next_obs, reward, termination, truncation, info = env.step(action)\n",
        "    done = termination or truncation\n",
        "\n",
        "    # store data and reset env if necessary\n",
        "    buffer.store(obs, next_obs, reward, action, done)\n",
        "    obs = next_obs\n",
        "    if done:\n",
        "        obs, _ = env.reset()\n",
        "\n",
        "    if step > learning_starts:\n",
        "        if step % behavior_net_update_freq == 0:\n",
        "            b_obs, b_next_obs, b_rewards, b_actions, b_dones = buffer.sample(batch_size)\n",
        "            ### your code goes below ###\n",
        "            # Implement the training code for optimizing the behavior network, you will need\n",
        "            # to leverage the target network to ensure training stability.\n",
        "            # Careful to note that in this homework for good performance you should ensure you don't\n",
        "            # create a temporal difference target that includes discounted values if it is the final frame (check via b_dones)\n",
        "            pass\n",
        "            ### your code goes above ###\n",
        "\n",
        "        if step % target_net_update_freq == 0:\n",
        "            ### your code goes below ###\n",
        "            # Implement the code to copy the target network. Aren't sure what to do? Check pytorch documentation!\n",
        "            pass\n",
        "            ### your code goes above ###\n",
        "\n",
        "    # every 10000 iterations evaluate once and save a video\n",
        "    if step % 10_000 == 0:\n",
        "        eval_eps_ret, eval_eps_len = evaluate(save_video=f\"cartpole_videos/{step}.mp4\")\n",
        "        pbar.set_postfix(dict(eval_eps_ret=eval_eps_ret, eval_eps_len=eval_eps_len, epsilon=get_epsilon(step)))\n",
        "        print(f\"step: {step}, eval_eps_ret: {eval_eps_ret}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjPFgJhwFRtt"
      },
      "outputs": [],
      "source": [
        "eval_env.reset(seed=0)\n",
        "avg_ret = 0\n",
        "trials = 20\n",
        "for trial in range(trials):\n",
        "    ret, _ = evaluate()\n",
        "    avg_ret += ret\n",
        "avg_ret /= trials\n",
        "print(\"Average Return: \", avg_ret)\n",
        "assert avg_ret >= 400, f\"agent does not perform well enough. Average return is {avg_ret} < 400, tune the hyperparameters/make sure DQN is implemented correctly\"\n",
        "P2_3_RET = avg_ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FsuOqpLFutr"
      },
      "outputs": [],
      "source": [
        "display_video(\"cartpole_videos/40000.mp4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQD086TyGUJD"
      },
      "source": [
        "# 3 Submission\n",
        "\n",
        "If you are taking the CSE 276F course at UC San Diego, to submit your work please save this .ipynb file locally first. Then run the cells below which zips up some of your answers and generated evaluation images/videos for grading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdi66_UAGVR3"
      },
      "outputs": [],
      "source": [
        "work_data = {\n",
        "    \"p1_1\": P1_1_PASS,\n",
        "    \"p2_1\": P2_1_PASS,\n",
        "    \"p2_2\": P2_2_PASS,\n",
        "    \"p2_3\": P2_3_RET\n",
        "}\n",
        "import json\n",
        "with open(\"answers.json\", 'w') as json_file:\n",
        "    json.dump(work_data, json_file, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9JIduHeG3Qu"
      },
      "outputs": [],
      "source": [
        "!mkdir -p hw_files/p1.3 2> /dev/null\n",
        "!cp P2_3.png hw_files/\n",
        "!cp cartpole_videos/40000.mp4 hw_files/\n",
        "!cp answers.json hw_files/\n",
        "!cp p_1_3_trial_*.mp4 hw_files/p1.3\n",
        "!tar -czvf hw_files.tar.gz hw_files > /dev/null"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "robolearning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0046d2d38f7841a1a3e6cca1453aa36e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f551383d0334b5faafffe973c2c90ee",
              "IPY_MODEL_cb4c13f69877491e9c86f51f4994e594",
              "IPY_MODEL_575332b205b84b089641fb4a3503d96d"
            ],
            "layout": "IPY_MODEL_ae418a7507bd4172a1c2b46a5b466a00"
          }
        },
        "010d0b81f97d4a93be83f60aa38ba601": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4506ddde1ad43468e512b66b0077a39",
            "placeholder": "​",
            "style": "IPY_MODEL_a1b34ccdbcea4ae5a7dd0b8b00e533e7",
            "value": "100%"
          }
        },
        "130b1ed3a097496fb0adea4ef48bed19": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13cc908517cb4bba98c00ef5b098cc86": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "160a04d02bc342f58acff4c1b6340463": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "32a95d9cd5ef436ba6d8fc847392a66c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b027b36b9a5748c3aae863acb0c247af",
            "max": 40000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bdd40b0f943448389276d18fdb74188a",
            "value": 40000
          }
        },
        "575332b205b84b089641fb4a3503d96d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13cc908517cb4bba98c00ef5b098cc86",
            "placeholder": "​",
            "style": "IPY_MODEL_667a2617238747e58a6ccd181c266cba",
            "value": " 50000/50000 [00:27&lt;00:00, 2465.94it/s, eval_eps_ret=398, eval_eps_len=398, epsilon=0.1]"
          }
        },
        "667a2617238747e58a6ccd181c266cba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "677e3400ddb241fba6a5107252e1b3b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79c35cd2e0ea403d9a94e98ccdb77f94",
            "placeholder": "​",
            "style": "IPY_MODEL_130b1ed3a097496fb0adea4ef48bed19",
            "value": " 40000/40000 [00:01&lt;00:00, 30462.00it/s]"
          }
        },
        "77fb60f7840c4abd8102c3baa2b0a740": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79c35cd2e0ea403d9a94e98ccdb77f94": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f551383d0334b5faafffe973c2c90ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77fb60f7840c4abd8102c3baa2b0a740",
            "placeholder": "​",
            "style": "IPY_MODEL_bf256bba7c924167955f49de330ffaae",
            "value": "100%"
          }
        },
        "a1b34ccdbcea4ae5a7dd0b8b00e533e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae418a7507bd4172a1c2b46a5b466a00": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b027b36b9a5748c3aae863acb0c247af": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b426bd2faa4241b789c7923789df845e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9163e69db824736ab0f7d354fbe2216": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_010d0b81f97d4a93be83f60aa38ba601",
              "IPY_MODEL_32a95d9cd5ef436ba6d8fc847392a66c",
              "IPY_MODEL_677e3400ddb241fba6a5107252e1b3b2"
            ],
            "layout": "IPY_MODEL_c395959e4c0d4a1ca1b566115d12b220"
          }
        },
        "bdd40b0f943448389276d18fdb74188a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf256bba7c924167955f49de330ffaae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c395959e4c0d4a1ca1b566115d12b220": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb4c13f69877491e9c86f51f4994e594": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b426bd2faa4241b789c7923789df845e",
            "max": 50000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_160a04d02bc342f58acff4c1b6340463",
            "value": 50000
          }
        },
        "d4506ddde1ad43468e512b66b0077a39": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
